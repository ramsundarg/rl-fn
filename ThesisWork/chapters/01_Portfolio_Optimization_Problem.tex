\chapter{Portfolio Optimization Problem}\label{chapter:PO_Problem}
In this chapter we begin by stating the Portfolio optimization problem. We then discuss some of the ways the portfolio optimization problem had been solved traditionally. We discuss some of the problems with respect to these approaches. 
We also discuss in brief some of the work done by our group in previous works.
Finally we outline our approach to solve this class of problems.

\section{Definition of the portfolio Optimization Problem} \label{section:POProblem}
We recap in this section the portfolio optimization problem as stated in an earlier work \cite{FernandezKschonnek2022}. Consider a finite time horizon $T > 0$ and a complete, filtered probability space
$(\Omega, \mathcal{F}_T , \mathbb{F} = (\mathcal{F}_{t})_{t \in [0,T]},Q)$ where the filtration $\mathbb{F}$ is generated by a Wiener process \cite{wiener1923differential} \\$W = (W(t))_{t\in[0,T]}.$\\

Consider a market model, $\mathcal{M}$ in which we have 2 assets, a risky asset which follows the Stochastic Differential Equation (SDE)

\begin{equation}
     dP_1(t) = P_1(t)(\mu dt + \sigma dW(t))
\end{equation}

 and a riskless asset which follows the SDE
 
\begin{equation}
     dP_0(t) = P_0(t)(r_c\text{ }dt).
\end{equation}

Here, $W(t)$ is the Wiener process we described earlier and the parameters $\mu$,$\sigma$ and $r_c$ are positive constants.

Let us assume that an agent in the market model specified by $\mathcal{M}$, trades in these 2 assets.  This means the agent needs to choose a portfolio process, $\pi : [0,T] \times (0,\infty) \rightarrow \mathbb{R}$, which maps the current time and the agent's current wealth $(t,v)$ to the proportion of the agent's wealth invested in the risky asset. We can state then that the evolution of the agent's wealth,$V^{v_0,\pi}$, in $\mathcal{M}$ satisfies the SDE 
\begin{equation} \label{equation:sde}
\begin{array}{l@{}l}
    dV^{v_0,\pi}(t) &{}= V^{v_0,\pi}(t) \mathbb{(}[r_c+(\mu-r_c)\pi(t,V^{v_0,\pi}(t))]dt + \pi(t,V^{v_0,\pi}(t))\sigma dW(t)\mathbb{)} \\
    V^{v_o,\pi}(0) &{}= v_0.
\end{array}
\end{equation}
Due to the Markovity of the setting, we assume that this relative portfolio process is a function of the current time and wealth $(t, V^{v_0,\pi}(t))$.
All $\pi$ satisfying some additional measurability and integrability conditions (see
equation (2.3) in \cite{Escobar-Anel2022} for details) will be called admissible and are collected in the set $\Lambda$.

We can find a semi-explicit solution for the SDE stated in (\ref{equation:sde}) as 
\begin{equation}
    V^{v_0,\pi}(t)  &{}= v_0\exp(\int_0^t r_c + (\mu-r_c)\pi(s,V^{v_0,\pi}(s)) -\frac{1}{2}(\sigma \pi ( s, V^{v_0,\pi}(s)))^2 ds
    +\int_0^t \pi(s,V^{v_0,\pi}(s))\sigma dW(s))
\end{equation}

Further, we assume that the agent is risk averse and derives utility from his terminal wealth at maturity $T$. Assume the utility function U is concave and an increasing function such that $U : [0,\infty) \rightarrow \mathbb{R} \cup \{-\infty\}$. The portfolio optimization problem is then defined as 
\begin{equation}
    (\mathbf{P})\begin{cases} \Phi(v_0) = \underset{\pi \in \Lambda}{\textit{sup }}\mathbb{ E}[U(V^{v_0,\pi}(T))]
    \end{cases}
\end{equation}

And the time-dependent version of the problem can be defined as 

\begin{equation}\label{equation:reduced_Po}
    (\mathbf{P_t}) \begin{cases}
        
     \Phi(t,v) = \underset{\pi \in \Lambda}{\textit{sup }}\mathbb{ E}[U(V^{v_0,\pi}(T)) | V^{v_0,\pi}(t)=v]
     \end{cases}
\end{equation}

There are quite a few approaches to solving this original portfolio optimization problem. We are listing some of the methods below.
\subsection{Martingale Method}
The portfolio optimization problem can be decomposed into a static optimization problem and a representation problem. The former involves determining the optimal terminal wealth, while the latter involves determining the corresponding optimal trading strategy. This approach, known as the martingale approach, is well-established in complete underlying financial market models, where it is supported by the Martingale Representation Theorem. To cite specific sources, refer to Chapter 3.4 in \cite{Korn1997} or Chapter 7.2 in \cite{Zagst2019}. However, for incomplete financial market models, such as the Heston model (see [17]), the martingale approach is not directly applicable. To address this, the financial market model can be artificially completed by adding a volatility-dependent derivative written on the risky asset, as discussed in \cite{Branger2008}, \cite{Branger2017}, \cite{Escobar2018}, and \cite{Liu2003}. 

\subsection{Merton's Method}

Merton's method of portfolio optimization \cite{Merton1969} regards the portfolio problem as a stochastic control problem, where the investor aims to maximize expected utility from consumption and terminal wealth. The method obtains the optimal value function, $\Phi (t,v)$ by solving a partial differential equation called the Hamilton-Jacobi-Bellman (HJB) equation \cite{korn2001optimal} \cite{davis1990portfolio}. The HJB equation is derived based on the principle of dynamic programming.

To solve the HJB equation, Merton's method requires an ansatz for the structure of the value function. Once the value function is obtained, the method derives the optimal trading strategy of the investor, which is a function of the portfolio's state variables - time $t$ and wealth $v$. 

\subsection{Challenges of these approaches}

The martingale approach, can be applied to characterize the optimal terminal wealth for a portfolio for many general utility functions and complete models (i.e the static problem is solvable under general assumptions) However obtaining the corresponding trading strategy (i.e solving the representation problem) is substantially more challenging in general settings.

Merton's method has the advantage of being able to handle incomplete financial models such as those with jump-diffusion processes and stochastic interest rates, but has the drawback of requiring the solution to the associated Hamilton–Jacobi–Bellman PDE. This can be challenging and time consuming especially in more complex settings, where there are no analytical solutions for this PDE.


\section{Reinforcement Learning for Portfolio Optimization and Trading Strategies}
\subsection{State of the Art}
After successes in solving games such as Atari \cite{mnih2015human}, Go, etc and also in its generic applicability to a  variety of fields such  as Natural Language Processing (NLP) (GPT models) \cite{brown2020language}, Reinforcement Learning (RL) is gaining considerable attention for its applications in finance. Several works have explored the potential of reinforcement learning techniques for financial portfolio management and trading strategies. We briefly look at some of them here.

One such work is "A deep Reinforcement learning framework for the financial portfolio management problem" by Ying et al. (2017) \cite{ying2017deep} which presents a deep reinforcement learning framework for portfolio management. They used the Deep Deterministic Policy Gradient (DDPG) algorithm, and their results showed that the proposed framework can learn effective portfolio management strategies in terms of metrics such as Sharpe ratio, portfolio returns and maximum draw down.

Another work in this field is "Application of deep reinforcement learning in stock trading strategies and stock forecasting" by Zhang et al. (2020) \cite{zhang2020application}. They proposed a deep reinforcement learning-based trading strategy that integrates technical analysis and fundamental analysis for stock trading. Their experimental results demonstrated that the proposed model can achieve better performance than other trading models in terms of the metrics such as Sharpe ratio, portfolio returns and maximum draw down.

In "Market Making via Reinforcement Learning" by Huang et al. (2018),\cite{huang2018market} the authors presented a market making algorithm that uses reinforcement learning to learn an optimal trading strategy. The algorithm is trained on a limit order book simulator and evaluated on a real-world dataset. The results showed that the algorithm can learn an effective market making strategy in terms of metrics such as effective bid-ask spread, participation rate, slippage ratio, and other portfolio metrics such as trading volume, Sharpe ratio, portfolio returns and maximum draw down etc..

Finally, "Deep Hedging" by Buehler et al. (2019) \cite{buehler2019deep} proposed a new method for hedging options in continuous time using deep learning. They demonstrated that the deep hedging approach can achieve better hedging performance than traditional methods.

\subsection{Advantages of Reinforcement Learning Techniques}
Some of the key areas where reinforcement learning techniques would fare better than traditional ways such as Merton's method and the martingale method described above are listed.
\begin{itemize}
    \item \textbf{Flexibility}: Reinforcement learning techniques can be applied to a wider range of optimization problems, without requiring assumptions on the utility functions such as concavity or differentiability.
    \item \textbf{Model-free}: Reinforcement learning techniques do not require an explicit model of the market dynamics or assumptions about the distribution of returns. This can be useful when the market model is complex and lacks analytical tractability, but can be used to sample market data efficiently.
    
   
\end{itemize}
\section{Previous Work and Project Outline}
Inspired by these advantages, previous projects in our group \cite{Janik2022} and \cite{RL_ArtInt_268}, worked on DDPG based approaches to solve this portfolio optimization problem using large neural networks to represent the "unknown" critic and actor. In that work, they obtained estimates for the Q-value function and the optimal allocation for logarithmic and power utility functions, which recovered the main characteristics of the true Q-value function and the true optimal allocation. 

However, despite achieving a reasonable estimation for the optimal allocation with both utility functions, a clear asymptotic convergence to the true optimal allocation could not be observed. Furthermore, the previous work used Tensorflow's \cite{tensorflow2015-whitepaper} implementation of DDPG and certain key parameters such as 
the magnitude of target-updates decay and (mini-) batch sizes increase during
DDPG’s run-time could not be configured. The algorithm's run time also exploded when the number of time steps in the time discretization increased.



\subsection{Project outline}
Considering the drawbacks from the previous projects, we identified the following areas of improvement, which constitute the main goals in our project.
\begin{itemize}
    \item \textbf{Specifying different function classes instead of neural networks for both Q-value and a-value function:} The motivation for moving away from an agnostic function and coming back to giving a specific structure to the functions (similar to the ansatz in Merton's method) is to incorporate known structural properties about the optimization problem into the reinforcement learning algorithm.
    \item \textbf{Build modular components:} To address the configurability problems with the previous work, we want to develop a modularized testing framework that can be utilized for different portfolio optimization problems by applying DDPG while experimenting with various specifications, such as and not limited to:
    \begin{itemize}
        \item Utility function (reward)
        \item Financial market model (environment)
        \item Parametrization of actor-function in DDPG
        \item Parametrization of critic-function in DDPG
    \end{itemize}
    \item \textbf{Tuning hyper-parameters of DDPG:}
        The framework should include a stand-alone implementation of DDPG so that changes to DDPG itself can be part of the modularization process.
    \item \textbf{Capabilities to analyze convergence speed and accuracy} 
    \item We aim to provide \textbf{detailed documentation} for the modularized testing framework so that this can be extended in future projects.
\end{itemize}




Overall, this thesis aims to develop a flexible and modular testing framework for portfolio optimization that can be adapted to various optimization problems. The performance of the DDPG algorithm in portfolio optimization will be tested under different utility functions and market models, and the results will be analyzed and compared to evaluate its accuracy and convergence speed. The framework will  be fully documented for the benefit of future users.





